---
title: "EMアルゴリズムを理解する"
emoji: "📊"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["AI", "音声", "アルゴリズム"]
published: false
---

# はじめに

EMアルゴリズム(Expectation-Maximization Algorithm)は、
潜在変数を持つ確率モデルのパラメータ推定を行うためのアルゴリズムです。
本記事では、EMアルゴリズムの基本的な考え方について解説します。

# EMアルゴリズムとは

EMアルゴリズムは、潜在変数を持つ確率モデルのパラメータ推定を行うためのアルゴリズムです。
潜在変数を持つ確率モデルとは、観測データの生成過程において、観測データのみならず、
観測データの潜在的な特徴を表す潜在変数も同時に生成される確率モデルのことを指します。
例えば、観測データが与えられたときに、潜在変数の事後分布を求める問題を考えます。
このような問題は、一般に解析的に解くことが難しいため、EMアルゴリズムを用いて数値的に解くことが一般的です。
EMアルゴリズムは、観測データの尤度関数を最大化するようにパラメータを更新することで、
潜在変数の事後分布を推定します。

具体的な手順は以下の通りです。
1. E-step: 現在のパラメータを使って、潜在変数の事後分布を推定する。
2. M-step: E-stepで求めた潜在変数の事後分布を使って、パラメータを更新する。
この手順を繰り返すことで、パラメータの推定値を収束させることができます。
EMアルゴリズムは、確率モデルのパラメータ推定だけでなく、
クラスタリングや密度推定など、様々な問題に応用されています。

# EMアルゴリズムのモデル

EMアルゴリズムは、以下のような確率モデルに適用されます。
観測データを$x$、潜在変数を$z$、パラメータを$\theta$とします。
この時、対数尤度$p(x|\theta)$は、次のように展開されます。

$$
\log p(x|\theta) = \log \sum_z p(x,z|\theta)\\
= \log \sum_z q(z) \frac{p(x,z|\theta)}{q(z)}\\
\geq \sum_z q(z) \log \frac{p(x,z|\theta)}{q(z)}\\
= \mathcal{L}(q, \theta)
$$

式中の不等号は、Jensenの不等式を用いています。
ここで、$q(z)$は潜在変数の近似事後分布を表し、
$\mathcal{L}(q, \theta)$は変分下限(Evidence Lower BOund)と呼ばれ、ELBOと略されます。
また、対数尤度とELBOとの差分は、

$$
\log p(x|\theta) - \mathcal{L}(q, \theta)\\
= \sum_z q(z) \log p(x|\theta) - \sum_z q(z) \log \frac{p(x,z|\theta)}{q(z)} \\
= \sum_z q(z) \log \frac{q(z)}{p(z|x,\theta)}\\
= KL(q(z)||p(z|x,\theta))
$$

よって、次の式が成り立ちます。

$$
\log p(x|\theta) = \mathcal{L}(q, \theta) + KL(q(z)||p(z|x,\theta))
$$

この式からわかる通り、対数尤度は変分下限とKLダイバージェンスの和として表されます。

**EMアルゴリズムでは、$p(z|x,\theta)$を解析的に計算できるという仮定**のもとで実施されます。
そのため、対数尤度を最大化すること











