---
title: "VAEの理解を深める"
emoji: "📚"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: ["機械学習", "深層学習", "生成モデル"]
published: true
---

# はじめに
この記事ではVAEの簡単な基本知識から、ベイズ推論的な視点や、その他の周辺知識について解説します。
対象者は、VAEの基本構造は知っているが、なぜそのような構造になっているのか、どのような考え方に基づいているのか、などの理解を深めたい人を想定しています。

# VAEとは
変分オートエンコーダ(Variational Auto Encoder; VAE)は、入力データの潜在的な特徴を表す潜在変数の分布を近似的に求めるための生成モデルです。
モデルのもととなる考え方は、変分ベイズ法に基づいており、入力データに対する潜在変数の事後分布を直接計算するのが困難な場合に、近似事後分布を導入して、その分布を用いて事後分布を近似するというものです。VAEはエンコーダとデコーダの2つのニューラルネットワークから構成されるため、オートエンコーダ(Auto Encoder; AE)の一種であり、入力データを圧縮することができます。しかし、主な目的は生成モデルとしての役割を果たすことです。
通常のAEと比較して、VAEは潜在変数を点推定ではなく、確率分布としてモデル化するため、潜在空間が断裂しておらず、滑らかであるという利点があります。したがって、新しいデータの生成に向いています。また、潜在変数の分布を正規分布などの既知の分布に制約することができるため、潜在空間の構造を制御しやすいという特徴も持っています。

# 基本概念
ベイズの定理では、潜在変数$\bm z$から観測データ$\bm x$をモデルパラメータ$\theta$の確率生成モデルによって予測するタスクにおいて、学習において用いる事後分布$p(\bm z|\bm x)$を提供します。ベイズの定理は次のように表されます。

$$
\begin{align*}
        p(\bm z|\bm x) 
        &= \frac{p_\theta(\bm x|\bm z)p(\bm z)}{p_\theta(\bm x)} \\
        &= \frac{p_\theta(\bm x|\bm z)p(\bm z)}{\int _{\bm z} p_\theta (\bm x | \bm z) p(\bm z) d \bm z}
\end{align*}
$$

混合ガウスモデルなどの確率生成モデルの最適化に用いられるEMアルゴリズムでは、潜在変数$\bm z$が離散的であり、Eステップにおいて解析的に事後分布$p(\bm z|\bm x)$を求め、Mステップにおいてモデルパラメータ$\theta$を最適化することが可能です。しかし、モデルの表現力には限界があり、$\bm z$から$\bm x$への生成プロセスが非常に単純であり、多様体仮説への適応能力も低いです。
VAEでは、ニューラルネットワークを用いて、$p_\theta(\bm x|\bm z)$をモデル化することで、より表現力の高い深層生成モデルを実現しています。深層生成モデルを用いる場合、潜在変数$\bm z$が高次元で連続的であるため、分母の尤度$p_\theta(\bm x)$の計算に必要な積分が事実上不可能となり、事後分布$p(\bm z|\bm x)$を解析的に求めることが困難になります。この問題を解決するために、VAEでは変分ベイズ法を用いて、モデルパラメータ$\varphi$のエンコーダによって近似的に事後分布$q_\varphi(\bm z|\bm x)$を予測します。ベイズの定理において、分母の$p_\theta(\bm x)$は計算不可能ですが、定数であるため、近似事後分布$q_\varphi(\bm z|\bm x)$は次のように比例関係で表されます。

$$
q_\varphi(\bm z|\bm x) \propto p_\theta(\bm x|\bm z)p(\bm z)
$$

よって、$q_\varphi(\bm z|\bm x)$は、潜在変数$\bm z$から観測データ$\bm x$を生成する確率モデル$p_\theta(\bm x|\bm z)$と、潜在変数$\bm z$の事前分布$p(\bm z)$の両方を考慮した分布となります。また、分布の形状自体は我々が定義した尤度関数$p_\theta(\bm x|\bm z)$と事前分布$p(\bm z)$によって即座に得られます。

:::details 共役事後に関して
事前分布と事後分布の関数の形が同じである場合、事後分布は共役であると呼ばれます。この関係にある場合、共役事後分布のパラメータは解析的に容易に計算できます。
:::

# モデル構造
![](/images/articles/vae_understanding/model.png)

VAEはエンコーダとデコーダの二つののDNNから構成されます。以降、エンコーダとデコーダのパラメータをそれぞれ$\varphi$, $\theta$と表します。
エンコーダでは入力データ$\bm x$から潜在変数$\bm z$に関する近似事後分布$q_\varphi(\bm z|\bm x)$を生成し、デコーダでは潜在変数$\bm z$から入力データ$\bm x$を予測するような尤度関数$p_\theta(\bm x|\bm z)$を生成します。デコーダへ入力する$\bm z$は、エンコーダで予測した近似事後分布$q_\varphi(\bm z|\bm x)$からreparameterization trickを用いてサンプリングされます。
$q_\varphi(\bm z|\bm x)$と$p_\theta(\bm x|\bm z)$の分布の形は、ここでは、両者とも多変量ガウス分布を仮定します。一般的にD次元の独立な多変量ガウス分布$N(\mu_\varphi, \Sigma_\varphi)$に定められる（平均場近似と呼ばれ、VAEでは多変量分布を各変数が独立であると仮定して表現する近似手法である。）ことが多く、共分散は0となる対角行列として表されます。

# 学習
確率生成モデルとしての役割を表すデコーダにおいて、最大化する目的関数は尤度$p_\theta(\bm x)$となります。尤度はエンコーダによって生成される近似事後分布$q_\varphi(\bm z|\bm x)$を用いて次のように表されます。

$$
\begin{align*}
    \ln p_\theta(\bm x) &= \ln \int_{\bm z} p_\theta(\bm x, \bm z) d\bm z \\
    &= \ln \int_{\bm z} q_\varphi(\bm z|\bm x) \frac{p_\theta(\bm x, \bm z)}{q_\varphi(\bm z|\bm x)} d\bm z \\
    &\geq \int_{\bm z} q_\varphi(\bm z|\bm x) \ln \frac{p_\theta(\bm x, \bm z)}{q_\varphi(\bm z|\bm x)} d\bm z \\
    &= \mathcal{L} _\text{ELBO}
\end{align*}
$$

第2行から第3行への変形はイェンゼンの不等式を用いています。この不等式から、対数尤度を最大化するためには、変分下限$\mathcal{L} _\text{ELBO}$を最大化すればよいことがわかります。また、不等式ではなく、尤度とELBOの差分を計算するなどの方法で恒等式として表し、整理すると次のようになります。

$$
\ln p_\theta(\bm x) - \text{KL}[q_\varphi(\bm z|\bm x) || p_\theta(\bm z|\bm x)] = \mathcal{L} _\text{ELBO}
$$

この式から、ELBOはデコーダに関する目的関数$\ln p_\theta(\bm x)$とエンコーダに関する目的関数$\text{KL}[q_\varphi(\bm z|\bm x) || p_\theta(\bm z|\bm x)]$の和であることがわかり、ELBOを最大化することはデコーダとエンコーダの両方の最適化を同時に行っていることに対応します。

ELBOを再度計算すると次のように表されます。

$$
\begin{align*}
\mathcal{L} _\text{ELBO} 
&= \int_{\bm z} q_\varphi(\bm z|\bm x) \ln \frac{p_\theta(\bm x, \bm z)}{q_\varphi(\bm z|\bm x)} d\bm z \\
&= \int_{\bm z} q_\varphi(\bm z|\bm x) \left[ \ln p_\theta(\bm x|\bm z) - \ln \frac{q_\varphi(\bm z|\bm x)}{p(\bm z)} \right] d\bm z \\
&= \mathbb{E}_{q_\varphi}[\ln p_\theta(\bm x|\bm z)] - \text{KL}[q_\varphi(\bm z|\bm x) || p(\bm z)]
\end{align*}
$$

この時、第1項目が再構成誤差、第2項目が正則化項の役割を果たしていると意味を取ることができます。再構成誤差に関しては、近似事後分布に従って得られた潜在変数から入力データを再構成する時、それがどれだけ尤もらしいかを表すことができます。正則化項に関しては、再構成誤差での学習で過剰適合することを防ぐために近似事後分布が事前分布から離れすぎないようにしていると意味を取ることができます。これを用いて逆伝播を行いVAEを学習します。

第1項目は度々モンテカルロ法によりサンプリング近似され、次のように表されます。ただし、$L$はサンプリング数とします。

$$
\mathbb{E}_{q_\varphi}[\ln p_\theta(\bm x|\bm z)] \approx \frac{1}{L} \sum_{l=1}^{L} \ln p_\theta(\bm x|\bm z^{(l)})
$$

# reparameterization trick
reparameterization trick は分布の統計的構造を次のネットワークに伝えるためのバイパスとして機能し、またサンプリング操作による計算グラフの断絶を防ぐ役割を果たします。VAEにおいては、エンコーダで生成される近似事後分布$q_\varphi(\bm z|\bm x)$の不確実性を考慮した学習が可能になります。$\bm z \sim q_\varphi(\bm z|\bm x) = N(\mu_\varphi, \Sigma_\varphi)$を満たす$\bm z'$を、ノイズ$\epsilon \sim N(0, I)$を用いた疑似サンプリングを考えると、次のように表されます。

$$
\bm z' = \mu_\varphi + \Sigma_\varphi \odot \epsilon
$$

この時、疑似サンプリングされた$\bm z' _d$の平均と分散を計算すると次のようになります。

$$
\mathbb{E}[\bm z' _d] = \mathbb{E}[\mu_{\varphi d} + \sigma_{\varphi d} \cdot \epsilon] = \mu_{\varphi d}
$$

$$
\mathbb{V}[\bm z' _d] = \mathbb{V}[\mu_{\varphi d} + \sigma_{\varphi d} \cdot \epsilon] = \sigma_{\varphi d}^2
$$

よって、疑似サンプリングされた$\bm z'$は近似事後分布$q_\varphi(\bm z|\bm x)$に従うことが確認できます。

reparameterization trickには以下に示す3つの重要な役割があります。
1. 計算グラフ断絶の防止 \
エンコーダによる$q_\varphi(\bm z|\bm x)$からの単なるサンプリング操作は確率的であり、計算グラフ上での終着点となってしまいます。これにより、デコーダ側で生じた誤差の勾配をエンコーダのパラメータ$\varphi$まで逆伝播させることができません。reparameterization trickを用いることで、サンプリング操作を決定論的な変換として表現し、計算グラフの連続性を保つことが可能になります。
2. 分布の構造情報の伝達 \
単なるサンプリングでは、デコーダに渡されるのは特定の一点のみであり、デコーダ側ではそれが確信度の高い（分散が小さい）中心付近の値なのか、不確実な（分散が大きい）外れ値なのかを区別する術がありません。しかし、reparameterization trickを経た$\bm z'$には、$\Sigma_\varphi$という広がりの情報がノイズ$\epsilon$の係数として埋め込まれています。デコーダがノイズによる揺らぎを含んだ$\bm z'$を受け取り、それら全てを元のデータ$\bm x$へと復元しようと反復的に試行する過程を通じて、単なる点ではなく、曖昧さを許容して広がりを持たせることで、分布を学習することが可能となります。
3. 潜在空間の動的マッピング \
reparameterization trickの式に関して、$\epsilon$を標準化の逆操作として解釈すると、この手法は標準正規分布という均一なノイズ空間$\epsilon$を、エンコーダが出力するパラメータによって意味を持つ潜在空間へと動的にマッピングする操作と解釈できます。$\mu_\varphi$は空間上の位置を、$\Sigma_\varphi$はその位置における空間の広がりを決定します。マッピングされた分布から得られるサンプルを反復的にデコーダへ渡すことでVAEは複雑なデータの構造を滑らかな潜在空間の広がりとして表現することに成功しています。

# 変分EMアルゴリズム
変分EMアルゴリズム(Variational EM algorithm; VEMアルゴリズム)は、従来のEMアルゴリズムを、潜在変数の事後分布が解析的に計算できない場合にも適用できるよう変分推論と合わせるように拡張した手法です。従来のEMアルゴリズムは、観測データに対する潜在変数の事後分布が解析的に計算可能な場合に適用されていました。Eステップで真の事後分布を正確に計算し、Mステップで対数尤度を最大化する手法であるため、学習率の調整が不要で、単調増加的に収束する安定性が利点です。対してVAEなどで用いられる変分推論は、事後分布が複雑で解析的に計算できない場合に適応されていました。ニューラルネットワークにおいてELBOを損失関数とし、誤差伝搬ですべてのパラメータを同時に更新する手法する手法であるため、柔軟性は高いですが、学習率の調整が必要で、解が振動する場合があります。

VEMアルゴリズムは、両者の特徴を組み合わせた手法です。VEMアルゴリズムでは、以下の2つのステップを交互に繰り返します。
1. 変分Eステップ：近似事後分布を更新するために、ELBOを最大化します。
2. Mステップ：固定された近似事後分布のもとで、モデルパラメータを更新します。

これにより、事後分布が計算できない複雑なモデルに対しても、EMアルゴリズムのような座標上昇法的（多変数関数の最大化において、一度に一つの変数のみを最適化し、他の変数を固定した状態で順次更新を繰り返すことで全体の最適解を求める反復的な計算手法のことです。）な安定した更新則を適用できる点が最大のメリットです。

# 参考文献
- D. P. Kingma and M. Welling, "Auto-Encoding Variational Bayes," in Proceedings of the 2nd International Conference on Learning Representations (ICLR), 2014.



